{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-данных\" data-toc-modified-id=\"Загрузка-данных-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Загрузка данных</a></span></li><li><span><a href=\"#Загрузка-стоп-слов-и-wordnet\" data-toc-modified-id=\"Загрузка-стоп-слов-и-wordnet-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Загрузка стоп-слов и wordnet</a></span></li><li><span><a href=\"#Предобработка-и-лемматизация-текста\" data-toc-modified-id=\"Предобработка-и-лемматизация-текста-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Предобработка и лемматизация текста</a></span></li><li><span><a href=\"#Токенизация-TF-IDF-и-разделение-на-выборки\" data-toc-modified-id=\"Токенизация-TF-IDF-и-разделение-на-выборки-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>Токенизация TF-IDF и разделение на выборки</a></span></li></ul></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Обучение-моделей-\" data-toc-modified-id=\"Обучение-моделей--2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Обучение моделей <br></a></span></li><li><span><a href=\"#Промежуточный-вывод-\" data-toc-modified-id=\"Промежуточный-вывод--2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Промежуточный вывод <br></a></span></li></ul></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span><ul class=\"toc-item\"><li><span><a href=\"#Комментарий-студента-\" data-toc-modified-id=\"Комментарий-студента--3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Комментарий студента <br></a></span></li></ul></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75.\n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели.\n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:16.042194Z",
     "start_time": "2024-12-02T08:27:16.039599Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as nltk_stopwords, wordnet\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import re\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:16.058153Z",
     "start_time": "2024-12-02T08:27:16.056546Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:16.584050Z",
     "start_time": "2024-12-02T08:27:16.064464Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    comments = pd.read_csv('../datasets/toxic_comments.csv')\n",
    "except FileNotFoundError:\n",
    "    comments = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "\n",
    "comments = comments.rename(columns={comments.columns[0]: 'index'})\n",
    "comments = comments.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:16.596407Z",
     "start_time": "2024-12-02T08:27:16.593041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159446</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159447</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159448</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159449</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159450</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "index                                                           \n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159446  \":::::And for the second time of asking, when ...      0\n",
       "159447  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159448  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159449  And it looks like it was actually you who put ...      0\n",
       "159450  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159292 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузка стоп-слов и wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:17.344005Z",
     "start_time": "2024-12-02T08:27:16.622338Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikitakhavkin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikitakhavkin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikitakhavkin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nikitakhavkin/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:17.364184Z",
     "start_time": "2024-12-02T08:27:17.362082Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка и лемматизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:27:17.392670Z",
     "start_time": "2024-12-02T08:27:17.390416Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Удаляем все, кроме букв и пробелов\n",
    "    text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', '', text)\n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Токенизация текста\n",
    "    words = word_tokenize(text)\n",
    "    # POS-тегирование\n",
    "    pos_tags = pos_tag(words)\n",
    "    # Удаляем стоп-слова и лемматизируем\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "        for word, _ in pos_tags\n",
    "        if word not in stopwords\n",
    "    ]\n",
    "    # Возвращаем предобработанный текст\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:00.058114Z",
     "start_time": "2024-12-02T08:27:17.524374Z"
    }
   },
   "outputs": [],
   "source": [
    "comments['text'] = comments['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация TF-IDF и разделение на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:00.078851Z",
     "start_time": "2024-12-02T08:34:00.069922Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    comments['text'], comments['toxic'], test_size=0.4, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:02.114117Z",
     "start_time": "2024-12-02T08:34:00.097600Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=list(stopwords))\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение моделей <br>\n",
    "Обучим 3 модели: логистическую регрессию, случайный лес и градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:02.118957Z",
     "start_time": "2024-12-02T08:34:02.117286Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=RANDOM_STATE)),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=RANDOM_STATE, n_estimators=50, learning_rate=0.2))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:02.129536Z",
     "start_time": "2024-12-02T08:34:02.125653Z"
    }
   },
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': uniform(0.01, 1),\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['saga']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(5, 50),\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        'n_estimators': randint(50, 200),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'max_depth': randint(3, 10)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:34:02.136285Z",
     "start_time": "2024-12-02T08:34:02.134752Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = 3\n",
    "best_model_name = None\n",
    "best_model = None\n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цикл для обучения моделей и вывода метрики F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:37:57.488532Z",
     "start_time": "2024-12-02T08:34:02.141900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "21 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.7101092  0.66339979 0.64649109        nan 0.72288945        nan\n",
      " 0.19712195 0.66366724 0.67337615 0.64928999 0.7296216         nan\n",
      "        nan        nan        nan 0.74847738        nan 0.73905611\n",
      " 0.56171185 0.68412712]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1-score for Logistic Regression (CV=3): 0.7485\n",
      "Best parameters for Logistic Regression: {'C': 0.9932308858067882, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m      2\u001B[0m     search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(\n\u001B[1;32m      3\u001B[0m         model,\n\u001B[1;32m      4\u001B[0m         param_distributions\u001B[38;5;241m=\u001B[39mparam_distributions[model_name],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     10\u001B[0m     )\n\u001B[0;32m---> 12\u001B[0m     search\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[1;32m     14\u001B[0m     mean_score \u001B[38;5;241m=\u001B[39m search\u001B[38;5;241m.\u001B[39mbest_score_\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest F1-score for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (CV=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmean_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1013\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[1;32m   1014\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[1;32m   1015\u001B[0m     )\n\u001B[1;32m   1017\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[0;32m-> 1019\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_search(evaluate_candidates)\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[1;32m   1023\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1960\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[0;34m(self, evaluate_candidates)\u001B[0m\n\u001B[1;32m   1958\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[1;32m   1959\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1960\u001B[0m     evaluate_candidates(\n\u001B[1;32m   1961\u001B[0m         ParameterSampler(\n\u001B[1;32m   1962\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_distributions, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_iter, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state\n\u001B[1;32m   1963\u001B[0m         )\n\u001B[1;32m   1964\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[0;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    958\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    960\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    961\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[1;32m    962\u001B[0m         )\n\u001B[1;32m    963\u001B[0m     )\n\u001B[0;32m--> 965\u001B[0m out \u001B[38;5;241m=\u001B[39m parallel(\n\u001B[1;32m    966\u001B[0m     delayed(_fit_and_score)(\n\u001B[1;32m    967\u001B[0m         clone(base_estimator),\n\u001B[1;32m    968\u001B[0m         X,\n\u001B[1;32m    969\u001B[0m         y,\n\u001B[1;32m    970\u001B[0m         train\u001B[38;5;241m=\u001B[39mtrain,\n\u001B[1;32m    971\u001B[0m         test\u001B[38;5;241m=\u001B[39mtest,\n\u001B[1;32m    972\u001B[0m         parameters\u001B[38;5;241m=\u001B[39mparameters,\n\u001B[1;32m    973\u001B[0m         split_progress\u001B[38;5;241m=\u001B[39m(split_idx, n_splits),\n\u001B[1;32m    974\u001B[0m         candidate_progress\u001B[38;5;241m=\u001B[39m(cand_idx, n_candidates),\n\u001B[1;32m    975\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_and_score_kwargs,\n\u001B[1;32m    976\u001B[0m     )\n\u001B[1;32m    977\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001B[38;5;129;01min\u001B[39;00m product(\n\u001B[1;32m    978\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(candidate_params),\n\u001B[1;32m    979\u001B[0m         \u001B[38;5;28menumerate\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrouted_params\u001B[38;5;241m.\u001B[39msplitter\u001B[38;5;241m.\u001B[39msplit)),\n\u001B[1;32m    980\u001B[0m     )\n\u001B[1;32m    981\u001B[0m )\n\u001B[1;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    984\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    987\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    988\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     73\u001B[0m )\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for model_name, model in models:\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_distributions[model_name],\n",
    "        scoring='f1',\n",
    "        cv=cv,\n",
    "        n_iter=20,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    mean_score = search.best_score_\n",
    "    print(f\"Best F1-score for {model_name} (CV={cv}): {mean_score:.4f}\")\n",
    "    print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
    "\n",
    "\n",
    "    if mean_score > best_score:\n",
    "        best_score = mean_score\n",
    "        best_model_name = model_name\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "print(f\"Best model: {best_model_name} with F1-score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка лучшей модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T08:37:57.503456Z",
     "start_time": "2024-12-01T18:32:42.948503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Logistic Regression\n",
      "Test F1-score for Logistic Regression: 0.7629\n"
     ]
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"Test F1-score for {best_model_name}: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Промежуточный вывод <br>\n",
    "Лучшая модель - логистическая регрессия с F1-score на тестовой выборке 0.7629 <br>\n",
    "F1-score больше 0.75, цель достигнута"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе исследования были выполнены следующие шаги:\n",
    "1. Загружены данные\n",
    "2. Загружены стоп-слова\n",
    "3. Проведена предобработка данных (токенизация)\n",
    "4. Обучены 3 модели: логистическая регрессия, случайный лес и градиентный бустинг\n",
    "5. Проведена оценка качества моделей по метрике F1\n",
    "6. Вывод: лучшее значение метрики показала модель логистической регрессии - 0.7629"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
